{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Points Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools as it\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "thirdparty_dir = os.path.join(os.getcwd(), '../thirdparty')\n",
    "sys.path.append(thirdparty_dir)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    img_lists = os.listdir(folder)\n",
    "    img_lists = sorted(img_lists, key=lambda p: int(Path(p).stem))\n",
    "    for filename in img_lists:\n",
    "        img = cv2.imread(os.path.join(folder, filename), cv2.IMREAD_COLOR)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "def scale_img(image, percent):\n",
    "    im = image\n",
    "    scale_percent = percent # percent of original size\n",
    "    width = int(im.shape[1] * scale_percent / 100)\n",
    "    height = int(im.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    im = cv2.resize(im, dim, interpolation = cv2.INTER_AREA)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '../data/datasets/television_dataset/masked_images'\n",
    "images = load_images_from_folder(images_path)\n",
    "sample_size = 4\n",
    "sequential = True\n",
    "plot_dpi = 300\n",
    "\n",
    "images_count = len(images)\n",
    "if sequential:\n",
    "    start = random.randint(0, images_count - sample_size)\n",
    "    taken_images_index = [*range(start, start + sample_size)]\n",
    "else:\n",
    "    taken_images_index = random.sample(range(images_count), sample_size if images_count > sample_size else images_count)\n",
    "print('Taken {} from {} images'.format(taken_images_index, images_count))\n",
    "\n",
    "sample_images = [images[i] for i in taken_images_index]\n",
    "gray_images = []\n",
    "\n",
    "for i in taken_images_index:\n",
    "    gray_image = cv2.cvtColor(images[i], cv2.COLOR_BGR2GRAY)\n",
    "    gray_images.append(gray_image)\n",
    "\n",
    "gray_image_index_pairs = [e for e in it.permutations(np.arange(len(gray_images)), 2)]\n",
    "print('Gray image pairs: {}'.format(gray_image_index_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samples(images):\n",
    "    fig, ax = plt.subplots(2, len(images), figsize=(14.5, 6), constrained_layout=True, sharey=True)\n",
    "    fig.set_dpi(plot_dpi)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        ax[0, i].set_title('Color Image {}'.format(i))\n",
    "        ax[0, i].imshow(images[i])\n",
    "        ax[1, i].set_title('Gray Image {}'.format(i))\n",
    "        ax[1, i].imshow(gray_images[i], cmap='gray')\n",
    "\n",
    "display_samples(sample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_img(image, percent):\n",
    "    im = image\n",
    "    scale_percent = percent # percent of original size\n",
    "    width = int(im.shape[1] * scale_percent / 100)\n",
    "    height = int(im.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    im = cv2.resize(im, dim, interpolation = cv2.INTER_AREA)\n",
    "    return im"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SIFT Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect Feature Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create()\n",
    "\n",
    "feature_points = []\n",
    "descriptors = []\n",
    "sift_time_ms = []\n",
    "\n",
    "for gray_image in gray_images:\n",
    "    b = datetime.datetime.now()\n",
    "    kps, des = sift.detectAndCompute(gray_image, None)\n",
    "    e = datetime.datetime.now()\n",
    "    \n",
    "    sift_time_ms.append((e - b).microseconds / 1000)\n",
    "    feature_points.append(kps)\n",
    "    descriptors.append(des)\n",
    "\n",
    "sift_time_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift_points_count = []\n",
    "\n",
    "def display_feature_points(images, points):\n",
    "    fig, ax = plt.subplots(1, len(images), figsize=(14.5, 6), constrained_layout=True, sharey=True)\n",
    "    fig.set_dpi(plot_dpi)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for i in range(0, len(images)):\n",
    "        sift_points_count.append(len(feature_points[i]))\n",
    "        ax[i].set_title('Extracted {} Points from Image {}'.format(len(feature_points[i]), i))\n",
    "        image_with_points = cv2.drawKeypoints(images[i], points[i], None)\n",
    "        ax[i].imshow(image_with_points)\n",
    "\n",
    "display_feature_points(gray_images, feature_points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching Feature Points using FLANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "flann_matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "flann_matches =[]\n",
    "flann_good_matches = []\n",
    "flann_time_ms = []\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "ratio_thresh = 0.8\n",
    "\n",
    "for (i, j) in gray_image_index_pairs:\n",
    "    good_matches = []\n",
    "\n",
    "    b = datetime.datetime.now()\n",
    "    matches = flann_matcher.knnMatch(descriptors[i], descriptors[j], k=2)\n",
    "    for m, n in matches:\n",
    "        if m.distance < ratio_thresh * n.distance:\n",
    "            good_matches.append(m)\n",
    "    e = datetime.datetime.now()\n",
    "    \n",
    "    flann_time_ms.append((e - b).microseconds / 1000)\n",
    "    flann_matches.append(matches)\n",
    "    flann_good_matches.append(good_matches)\n",
    "\n",
    "flann_time_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flann_matches_count = [(len(gm), len(am)) for (gm, am) in zip(flann_good_matches, flann_matches)]\n",
    "\n",
    "def display_matches_quality(index_pairs, good_matches, all_matches, n=20):\n",
    "    good_matches_sorted = [sorted(gm, key=lambda val: val.distance) for gm in good_matches]\n",
    "    distances = [[m.distance for m in gm_sorted] for gm_sorted in good_matches_sorted]\n",
    "    percentages = [(len(gm_sorted) / len(am)) for (gm_sorted, am) in zip(good_matches_sorted, all_matches)]\n",
    "    \n",
    "    plt.figure(figsize=(14.5, 6), dpi=plot_dpi)\n",
    "    plt.title('{} Matches of Image Pairs, Matches Utilization Average {:.3f}%'.format(len(good_matches), np.average(percentages) * 100))\n",
    "    plt.xticks(range(0, n, 3))\n",
    "    plt.xlabel('Matches Index')\n",
    "    plt.ylabel('Euclidean Distance (pixels)')\n",
    "\n",
    "    for (ip, y, p) in zip(index_pairs, distances, percentages):\n",
    "        x = np.arange(0, len(y))\n",
    "        plt.plot(x[:n], y[:n], linestyle='--', marker='o', label='Pair {}, {:.3f}%'.format(ip, p * 100))\n",
    "        plt.legend()\n",
    "\n",
    "display_matches_quality(gray_image_index_pairs, flann_good_matches, flann_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_matches(index_pairs, gray_images, feature_points, good_matches, n=2):\n",
    "    fig, ax = plt.subplots(1, n, figsize=(14.5, 6), constrained_layout=True, sharey=True)\n",
    "    fig.set_dpi(plot_dpi)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for ((i, j), k) in zip(index_pairs[:n], range(0, n)):\n",
    "        stitch_image = np.empty(\n",
    "            (max(gray_images[i].shape[0], gray_images[j].shape[0]), gray_images[i].shape[1] + gray_images[j].shape[1], 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        cv2.drawMatches(\n",
    "            gray_images[i], feature_points[i],\n",
    "            gray_images[j], feature_points[j],\n",
    "            good_matches[k], stitch_image,\n",
    "            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "        )\n",
    "\n",
    "        ax[k].set_title('{} Matches in Image Pair {}'.format(len(good_matches[k]), (i, j)))\n",
    "        ax[k].imshow(stitch_image)\n",
    "\n",
    "display_matches(gray_image_index_pairs, gray_images, feature_points, flann_good_matches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SuperPoint + SuperGlue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect Feature Points using SuperPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Running inference on device \\\"{}\\\"'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SuperPointPretrainedNetwork.superpoint import SuperPoint\n",
    "\n",
    "superpoint_conf = {\n",
    "    'descriptor_dim': 256,\n",
    "    'nms_radius': 3,\n",
    "    'max_keypoints': 4096,\n",
    "    'keypoints_threshold': 0.6\n",
    "}\n",
    "\n",
    "sp_model = SuperPoint(superpoint_conf)\n",
    "sp_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_sp_model = torch.load(\n",
    "    '../thirdparty/SuperPointPretrainedNetwork/superpoint_v1.pth',\n",
    "    map_location=device\n",
    ")\n",
    "sp_model.load_state_dict(pretrained_sp_model, strict=True)\n",
    "\n",
    "if device == 'cuda':\n",
    "    sp_model = sp_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_dummpy_input = np.random.random_sample((1, 1, gray_images[0].shape[0], gray_images[0].shape[1])).astype(np.float32)\n",
    "_ = sp_model(torch.from_numpy(sp_dummpy_input).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_tensor(image):\n",
    "    normalized = image.astype('float32') / 255.\n",
    "    tensor = torch.from_numpy(normalized).to(device).unsqueeze(0).unsqueeze(0)\n",
    "    return tensor\n",
    "\n",
    "normalized_gray_images = [normalize_to_tensor(img) for img in gray_images]\n",
    "\n",
    "print(gray_images[0].shape)\n",
    "print(normalized_gray_images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_predicts = []\n",
    "sp_time_ms =[]\n",
    "\n",
    "for t in normalized_gray_images:\n",
    "    b = datetime.datetime.now()\n",
    "    pred = sp_model(t)\n",
    "    e = datetime.datetime.now()\n",
    "\n",
    "    sp_time_ms.append((e - b).microseconds / 1000)\n",
    "    sp_predicts.append(pred)\n",
    "\n",
    "print(sp_predicts[0].keys())\n",
    "print(sp_time_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = [p['keypoints'][0].detach().cpu().numpy() for p in sp_predicts]\n",
    "scores = [p['scores'][0].detach().cpu().numpy() for p in sp_predicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_points_count = []\n",
    "\n",
    "def display_feature_points(images, keypoints, scores):\n",
    "    fig, ax = plt.subplots(1, len(images), figsize=(16.5, 2.8), constrained_layout=True, sharey=True)\n",
    "    fig.set_dpi(plot_dpi)\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(cm.ScalarMappable(cmap=cm.hot), ax=fig.get_axes(), orientation='vertical', label='Scores', pad=0.01)\n",
    "\n",
    "    for i in range(0, len(images)):\n",
    "        out_img = images[i].copy()\n",
    "        out_img = cv2.cvtColor(out_img, cv2.COLOR_GRAY2RGB)\n",
    "        kpts = np.round(keypoints[i]).astype('int')\n",
    "        sp_points_count.append(len(kpts))\n",
    "        \n",
    "        for (x, y), s in zip(kpts, scores[i]):\n",
    "            color = (np.array(cm.hot(s)) * 255).astype('uint8')\n",
    "            cv2.circle(out_img, (x, y), 3, color.tolist(), 1, lineType=cv2.LINE_AA)\n",
    "\n",
    "        ax[i].set_title('Extracted {} Points from Image {}'.format(len(kpts), i))\n",
    "        ax[i].imshow(out_img)\n",
    "\n",
    "display_feature_points(gray_images, keypoints, scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching Feature Point using SuperGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SuperGluePretrainedNetwork.models.superglue import SuperGlue\n",
    "\n",
    "superglue_conf =  {\n",
    "    'descriptor_dim': 256,\n",
    "    'weights': 'indoor',\n",
    "    'match_threshold': 0.7\n",
    "}\n",
    "\n",
    "sg_model = SuperGlue(superglue_conf)\n",
    "sg_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_sg_model = torch.load(\n",
    "    '../thirdparty/SuperGluePretrainedNetwork/models/weights/superglue_{}.pth'.format(superglue_conf['weights']),\n",
    "    map_location=device\n",
    ")\n",
    "sg_model.load_state_dict(pretrained_sg_model, strict=True)\n",
    "\n",
    "if device == 'cuda':\n",
    "    sg_model = sg_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sg_data(index_pairs, gray_images, sp_predicts):\n",
    "    i, j = index_pairs\n",
    "    feats0, feats1 = sp_predicts[i], sp_predicts[j]\n",
    "    img_size0, img_size1 = gray_images[i].shape, gray_images[j].shape\n",
    "    data = {}\n",
    "\n",
    "    for k in feats0.keys():\n",
    "        data[k + '0'] = feats0[k][0]\n",
    "    for k in feats1.keys():\n",
    "        data[k + '1'] = feats1[k][0]\n",
    "\n",
    "    data['image0'] = torch.empty((1, 1, ) + tuple(img_size0)[:-1]).to(device)\n",
    "    data['image1'] = torch.empty((1, 1, ) + tuple(img_size1)[:-1]).to(device)\n",
    "    data = {k: v[None].float().to(device) for k, v in data.items()}\n",
    "\n",
    "    return data\n",
    "    \n",
    "sg_datas = [generate_sg_data(p, gray_images, sp_predicts) for p in gray_image_index_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_predicts = []\n",
    "sg_time_ms = []\n",
    "\n",
    "for inp in sg_datas:\n",
    "    b = datetime.datetime.now()\n",
    "    pred = sg_model(inp)\n",
    "    e = datetime.datetime.now()\n",
    "\n",
    "    sg_time_ms.append((e - b).microseconds / 1000)\n",
    "    sg_predicts.append(pred)\n",
    "    \n",
    "sg_time_ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_matches_count = []\n",
    "\n",
    "def display_matches_scores(index_pairs, sg_predicts, n=20):\n",
    "    percentages = []\n",
    "\n",
    "    plt.figure(figsize=(14.5, 6), dpi=plot_dpi)\n",
    "    plt.xticks(range(0, n, 3))\n",
    "    plt.xlabel('Matches Index')\n",
    "    plt.ylabel('Confidence (%)')\n",
    "\n",
    "    for ((i, j), pred) in zip(index_pairs, sg_predicts):\n",
    "        matches = pred['matches0'][0].detach().cpu().numpy()\n",
    "        valid = matches > -1\n",
    "        valid_scores = pred['matching_scores0'][0].detach().cpu().numpy()[valid]\n",
    "        valid_scores_sorted = sorted(valid_scores, reverse=True)\n",
    "        percentage = len(valid_scores_sorted) / len(matches)\n",
    "        sg_matches_count.append((len(valid_scores), len(matches)))\n",
    "\n",
    "        percentages.append(percentage)\n",
    "\n",
    "        x = np.arange(0, len(valid_scores_sorted))\n",
    "        y = valid_scores_sorted\n",
    "\n",
    "        plt.plot(x[:n], y[:n], linestyle='--', marker='o', label='Pair {}, {:.3f}%'.format((i, j), percentage * 100))\n",
    "        plt.legend()\n",
    "\n",
    "    plt.title('{} Matches of Image Pairs, Matches Utilization Average {:.3f}%'.format(len(sg_predicts), np.average(percentages) * 100))\n",
    "\n",
    "display_matches_scores(gray_image_index_pairs, sg_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_matches(index_pairs, gray_images, sp_predicts, sg_predicts, n=2):\n",
    "    fig, ax = plt.subplots(1, n, figsize=(15.5, 2.8), constrained_layout=True, sharey=True)\n",
    "    fig.set_dpi(plot_dpi)\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(cm.ScalarMappable(cmap=cm.hot), ax=fig.get_axes(), orientation='vertical', label='Scores', pad=0.01)\n",
    "\n",
    "    for ((i, j), sg_pred, k) in zip(index_pairs[:n], sg_predicts[:n], range(0, n)):\n",
    "        matches = sg_pred['matches0'][0].detach().cpu().numpy()\n",
    "        valid = matches > -1\n",
    "\n",
    "        feats0, feats1 = sp_predicts[i], sp_predicts[j]\n",
    "        kpts0, kpts1 = feats0['keypoints'][0].detach().cpu().__array__(), feats1['keypoints'][0].detach().cpu().__array__()\n",
    "        mkpts0, mkpts1 = np.round(kpts0[valid]).astype('int'), np.round(kpts1[matches[valid]]).astype('int')\n",
    "\n",
    "        valid_scores = sg_pred['matching_scores0'][0].detach().cpu().numpy()[valid]\n",
    "        percentage = len(valid_scores) / len(matches)\n",
    "\n",
    "        (h0, w0), (h1, w1) = gray_images[i].shape, gray_images[j].shape\n",
    "        stitch_image = np.empty((max(h0, h1), w0 + w1), dtype=np.uint8)\n",
    "        stitch_image[:h0, :w0] = gray_images[i]\n",
    "        stitch_image[:h1, w0:] = gray_images[j]\n",
    "        stitch_image = cv2.cvtColor(stitch_image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        for (x0, y0), (x1, y1), s in zip(mkpts0, mkpts1, valid_scores):\n",
    "            color = (np.array(cm.hot(s)) * 255).astype('uint8')\n",
    "\n",
    "            cv2.line(stitch_image, (x0, y0), (x1 + w0, y1), color.tolist(), thickness=1, lineType=cv2.LINE_AA)\n",
    "            cv2.circle(stitch_image, (x0, y0), 3, color.tolist(), 1, lineType=cv2.LINE_AA)\n",
    "            cv2.circle(stitch_image, (x1 + w0, y1), 3, color.tolist(), 1, lineType=cv2.LINE_AA)\n",
    "        \n",
    "        ax[k].set_title('{} Matches in Image Pair {}'.format(len(valid_scores), (i, j)))\n",
    "        ax[k].imshow(stitch_image)\n",
    "\n",
    "display_matches(gray_image_index_pairs, gray_images, sp_predicts, sg_predicts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_time_consumption(durations_with_name):\n",
    "    total_width, n = 0.8, len(durations_with_name)\n",
    "    width = total_width / n\n",
    "    x_max = 0\n",
    "    cmap = cm.get_cmap()\n",
    "\n",
    "    plt.figure(figsize=(14.5, 5), dpi=plot_dpi)\n",
    "    plt.xlabel('Measurement Index')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.title('Time Consumption for Each Processing Step')\n",
    "\n",
    "    for ((y, t), i) in zip(durations_with_name, range(0, n)):\n",
    "        x = np.arange(0, len(y))\n",
    "        x_max = max(len(y), x_max)\n",
    "        avg = np.average(y)\n",
    "        color = cmap(y * 255)\n",
    "        \n",
    "        bar = plt.bar(x + (i * width - (total_width - width) / 2), y, width=width, label='{}, average {:.3f}ms'.format(t, avg))\n",
    "        axhline = plt.axhline(avg, color=bar.patches[0].get_facecolor(), linestyle='--')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.xticks(range(0, x_max, 1))\n",
    "    plt.yscale('function', functions=(lambda v: v ** 0.5, lambda v: v ** 2))\n",
    "\n",
    "durations_with_name = [\n",
    "    (sift_time_ms, 'SIFT'),\n",
    "    (flann_time_ms, 'FLANN'),\n",
    "    (sp_time_ms, 'SuperPoint (CPU, M1)'),\n",
    "    (sg_time_ms, 'SuperGlue (CPU, M1)')\n",
    "]\n",
    "\n",
    "display_time_consumption(durations_with_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Points Quantity and Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_quantity_and_utilization(points_counts, matches_counts):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14.5, 3), gridspec_kw={'width_ratios': [1, 2]}, constrained_layout=True, sharey=True)\n",
    "    fig.set_dpi(plot_dpi)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    ax[0].set_title('Extracted Feature Points')\n",
    "    ax[0].set_xticks(range(0, len(points_counts[0][0]), 1))\n",
    "    ax[0].set_xlabel('Image Index')\n",
    "    ax[0].set_ylabel('Number of Points')\n",
    "    for (pc, lb) in points_counts:\n",
    "        x = range(0, len(pc))\n",
    "        l = ax[0].plot(x, pc, linestyle=':', marker='o', label=lb)\n",
    "        h = ax[0].axhline(np.average(pc), color=l[0].get_color(), linestyle='--')\n",
    "        ax[0].legend()\n",
    "\n",
    "    ax[1].set_title('Matched Points Pairs')\n",
    "    ax[1].set_xlabel('Image Pair Index')\n",
    "    ax[1].set_xticks(range(0, len(matches_counts[0][0]), 1))\n",
    "    ax[1].set_ylabel('Matches')\n",
    "    ax_t = ax[1].twinx()\n",
    "    ax_t.set_ylabel('Utilization (%)')\n",
    "    ps_max = 0\n",
    "    for (mc, lb) in matches_counts:\n",
    "        x = range(0, len(mc))\n",
    "        v = [f for (f, _) in mc]\n",
    "        a = [s for (_, s) in mc]\n",
    "        ps = [(vc / ac) * 100 for (vc, ac) in zip(v, a)]\n",
    "        ps_max = ps_max if ps_max > max(ps) else max(ps)\n",
    "        lv = ax[1].plot(x, v, linestyle=':', marker='o', label=lb)\n",
    "        la = ax[1].plot(x, a, linestyle='', marker='x', color=lv[0].get_color())\n",
    "        lp = ax_t.plot(x, ps, linestyle='', marker='s', color=lv[0].get_color())\n",
    "        lh = ax_t.axhline(np.average(ps), linestyle='--', color=lv[0].get_color())\n",
    "        ax[1].legend()\n",
    "    ax_t.set_yticks(range(0, int(ps_max), 2))\n",
    "\n",
    "points_counts = [\n",
    "    (sift_points_count, 'SIFT'),\n",
    "    (sp_points_count, 'SuperPoint')\n",
    "]\n",
    "matches_counts = [\n",
    "    (flann_matches_count, 'FLANN'),\n",
    "    (sg_matches_count, 'SuperGlue')\n",
    "]\n",
    "\n",
    "display_quantity_and_utilization(points_counts, matches_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of Future Points Extracting and Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homography_estimation = {\n",
    "    'SIFT': {\n",
    "        '1': 0.63,\n",
    "        '3': 0.76,\n",
    "        '5': 0.79\n",
    "    },\n",
    "    'SuperPoint (Pretrained)': {\n",
    "        '1': 0.44,\n",
    "        '3': 0.77,\n",
    "        '5': 0.83\n",
    "    },\n",
    "    'SuperPoint (COCO)': {\n",
    "        '1': 0.46,\n",
    "        '3': 0.75,\n",
    "        '5': 0.81\n",
    "    }\n",
    "}\n",
    "\n",
    "detector_metric = {\n",
    "    'SIFT': {\n",
    "        'Repeatability': 0.51,\n",
    "        'MLE': 1.16\n",
    "    },\n",
    "    'SuperPoint (Pretrained)': {\n",
    "        'Repeatability': 0.61,\n",
    "        'MLE': 1.14\n",
    "    },\n",
    "    'SuperPoint (COCO)': {\n",
    "        'Repeatability': 0.63,\n",
    "        'MLE': 1.07\n",
    "    }\n",
    "}\n",
    "\n",
    "descriptor_metric = {\n",
    "    'SIFT': {\n",
    "        'NN mAP': 0.80,\n",
    "        'Matching Score': 0.27\n",
    "    },\n",
    "    'SuperPoint (Pretrained)': {\n",
    "        'NN mAP': 0.81,\n",
    "        'Matching Score': 0.55\n",
    "    },\n",
    "    'SuperPoint (COCO)': {\n",
    "        'NN mAP': 0.78,\n",
    "        'Matching Score': 0.42\n",
    "    }\n",
    "}\n",
    "\n",
    "# From https://github.com/eric-yyjau/pytorch-superpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_overall_quality(homography, detector, descriptor):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(14.5, 3), constrained_layout=True, sharey=True)\n",
    "    fig.set_dpi(plot_dpi)\n",
    "    fig.tight_layout(pad=3)\n",
    "\n",
    "    ax[0].set_title('Homography')\n",
    "    ax[0].set_xlabel('Epsilon')\n",
    "    ax[0].set_ylabel('Estimation')\n",
    "    for (k, v) in homography.items():\n",
    "        y = [e for (_, e) in v.items()]\n",
    "        x = [e for (e, _) in v.items()]\n",
    "        ax[0].plot(x, y, linestyle=':', marker='o', label=k)\n",
    "        ax[0].legend()\n",
    "\n",
    "    ax[1].set_title('Detector Metric')\n",
    "    ax[1].set_xlabel('Name')\n",
    "    ax[1].set_ylabel('Repeatability / MLE')\n",
    "    x = [n for (n, _) in detector.items()]\n",
    "    ys = {\n",
    "        'Repeatability': [p['Repeatability'] for (_, p) in detector.items()],\n",
    "        'MLE': [p['MLE'] for (_, p) in detector.items()]\n",
    "    }\n",
    "    for (k, y) in ys.items():\n",
    "        ax[1].plot(x, y, linestyle='', marker='x', label=k)\n",
    "        ax[1].legend()\n",
    "    \n",
    "    ax[2].set_title('Descriptor Metric')\n",
    "    ax[2].set_xlabel('Name')\n",
    "    ax[2].set_ylabel('NN mAP / Matching Score')\n",
    "    ys = {\n",
    "        'NN mAP': [p['NN mAP'] for (_, p) in descriptor.items()],\n",
    "        'Matching Score': [p['Matching Score'] for (_, p) in descriptor.items()]\n",
    "    }\n",
    "    for (k, y) in ys.items():\n",
    "        ax[2].plot(x, y, linestyle='', marker='x', label=k)\n",
    "        ax[2].legend()\n",
    "\n",
    "\n",
    "display_overall_quality(homography_estimation, detector_metric, descriptor_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SfM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5354608d47e8d3d40283fd6505b86116d9905b298f75dd83c53ff0767e8880c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
